{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sadyk703/Practical_RL_1/blob/master/DRL_sem_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i7omziuSk9Dc"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "mMykIckHkwri",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d33f8a5b-134b-40cd-9f4e-79720d686791"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/MattChanTK/gym-maze\n",
            "  Cloning https://github.com/MattChanTK/gym-maze to /tmp/pip-req-build-lpl8gij7\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/MattChanTK/gym-maze /tmp/pip-req-build-lpl8gij7\n",
            "  Resolved https://github.com/MattChanTK/gym-maze to commit 83176811b49b5538a6213520612f44fb1bc49114\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: gym in /usr/local/lib/python3.10/dist-packages (from gym-maze==0.4) (0.25.2)\n",
            "Requirement already satisfied: pygame in /usr/local/lib/python3.10/dist-packages (from gym-maze==0.4) (2.5.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from gym-maze==0.4) (1.25.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gym->gym-maze==0.4) (2.2.1)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym->gym-maze==0.4) (0.0.8)\n",
            "Building wheels for collected packages: gym-maze\n",
            "  Building wheel for gym-maze (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gym-maze: filename=gym_maze-0.4-py3-none-any.whl size=14214 sha256=9e2e1779bfc21f8c3dea5bbd5ff9693ffe7ec29426e0e691119d839f1a0ccf40\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-7wvz933n/wheels/95/29/73/aefced63758e3f256e0eb5f5f9d37a7e6ca3d6d072d8803f6a\n",
            "Successfully built gym-maze\n",
            "Installing collected packages: gym-maze\n",
            "Successfully installed gym-maze-0.4\n"
          ]
        }
      ],
      "source": [
        "!pip install git+https://github.com/MattChanTK/gym-maze"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "XwGvKHN7rTod"
      },
      "outputs": [],
      "source": [
        "import gym\n",
        "import gym_maze\n",
        "import numpy as np\n",
        "import random\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "I6EtIziirXUT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b0a14cf5-1269-4c10-90c6-e02c777b3faa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n"
          ]
        }
      ],
      "source": [
        "env = gym.make('maze-sample-5x5-v0')\n",
        "state_n =25\n",
        "action_n =4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "SZ_9Rrl9sJh0"
      },
      "outputs": [],
      "source": [
        "class RandomAgent():\n",
        "    def __init__(self, action_n):\n",
        "        self.action_n = action_n\n",
        "\n",
        "    def get_action(self, state):\n",
        "        action = np.random.randint(self.action_n)\n",
        "        return action"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "vXuqjKKNs0J7"
      },
      "outputs": [],
      "source": [
        "class CrossEntropyAgent():\n",
        "    def __init__(self, state_n, action_n):\n",
        "        self.state_n = state_n\n",
        "        self.action_n = action_n\n",
        "        self.model = np.ones((self.state_n, self.action_n)) / self.action_n\n",
        "\n",
        "    def get_action(self, state):\n",
        "        action = np.random.choice(np.arange(self.action_n), p=self.model[state])\n",
        "        return int(action)\n",
        "\n",
        "    def fit(self, elite_trajectories):\n",
        "        new_model = np.zeros((self.state_n, self.action_n))\n",
        "        for trajectory in elite_trajectories:\n",
        "            for state, action in zip(trajectory['states'], trajectory['actions']):\n",
        "                new_model[state][action] += 1\n",
        "\n",
        "        for state in range(self.state_n):\n",
        "            if np.sum(new_model[state]) > 0:\n",
        "                new_model[state] /= np.sum(new_model[state])\n",
        "            else:\n",
        "                new_model[state] = self.model[state].copy()\n",
        "\n",
        "        self.model = new_model\n",
        "        return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "m52JHbyrsO-c"
      },
      "outputs": [],
      "source": [
        "def get_state(obs):\n",
        "    return int(np.sqrt(state_n) * obs[0] + obs[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "LC20EhRpwdiL"
      },
      "outputs": [],
      "source": [
        "def get_trajectory(env, agent, max_len=1000, visualize=True):\n",
        "    trajectory = {'states': [], 'actions': [], 'rewards': []}\n",
        "\n",
        "    obs = env.reset()\n",
        "    state = get_state(obs)\n",
        "\n",
        "    for _ in range(max_len):\n",
        "        trajectory['states'].append(state)\n",
        "\n",
        "        action = agent.get_action(state)\n",
        "        trajectory['actions'].append(action)\n",
        "\n",
        "        obs, reward, done, _ = env.step(action)\n",
        "        trajectory['rewards'].append(reward)\n",
        "\n",
        "        state = get_state(obs)\n",
        "\n",
        "        if visualize:\n",
        "            time.sleep(0.5)\n",
        "            env.render()\n",
        "\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "    return trajectory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "qp88aU8-xHWk"
      },
      "outputs": [],
      "source": [
        "agent = CrossEntropyAgent(state_n, action_n)\n",
        "q_param = 0.9\n",
        "iteration_n = 20\n",
        "trajectory_n = 50"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "RbzHEn7ixKyS",
        "outputId": "a4e179c5-b5ea-4465-de44-19ac29c4835f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:227: DeprecationWarning: \u001b[33mWARN: Core environment is written in old step API which returns one bool instead of two. It is recommended to rewrite the environment with new step API. \u001b[0m\n",
            "  logger.deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
            "  if not isinstance(done, (bool, np.bool8)):\n",
            "/usr/local/lib/python3.10/dist-packages/gym/core.py:49: DeprecationWarning: \u001b[33mWARN: You are calling render method, but you didn't specified the argument render_mode at environment initialization. To maintain backward compatibility, the environment will render in human mode.\n",
            "If you want to render in human mode, initialize the environment in this way: gym.make('EnvName', render_mode='human') and don't call the render method.\n",
            "See here for more information: https://www.gymlibrary.ml/content/api/\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:280: UserWarning: \u001b[33mWARN: No render modes was declared in the environment (env.metadata['render_modes'] is None or not defined), you may have trouble when calling `.render()`.\u001b[0m\n",
            "  logger.warn(\n"
          ]
        }
      ],
      "source": [
        "for iteration in range(iteration_n):\n",
        "\n",
        "    #policy evaluation\n",
        "    trajectories = [get_trajectory(env, agent) for _ in range(trajectory_n)]\n",
        "    total_rewards = [np.sum(trajectory['rewards']) for trajectory in trajectories]\n",
        "    print('iteration:', iteration, 'mean total reward:', np.mean(total_rewards))\n",
        "\n",
        "    #policy improvement\n",
        "    quantile = np.quantile(total_rewards, q_param)\n",
        "    elite_trajectories = []\n",
        "    for trajectory in trajectories:\n",
        "        total_reward = np.sum(trajectory['rewards'])\n",
        "        if total_reward > quantile:\n",
        "            elite_trajectories.append(trajectory)\n",
        "\n",
        "    agent.fit(elite_trajectories)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "ov661uGYxZDj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6c1a9f9f-7a1b-4223-d5b8-8315135c7fe7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:174: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed a `seed` instead of using `Env.seed` for resetting the environment random number generator.\u001b[0m\n",
            "  logger.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:190: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed `return_info` to return information from the environment resetting.\u001b[0m\n",
            "  logger.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:195: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed `options` to allow the environment initialisation to be passed additional information.\u001b[0m\n",
            "  logger.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:141: UserWarning: \u001b[33mWARN: The obs returned by the `reset()` method was expecting numpy array dtype to be int64, actual type: float64\u001b[0m\n",
            "  logger.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:165: UserWarning: \u001b[33mWARN: The obs returned by the `reset()` method is not within the observation space.\u001b[0m\n",
            "  logger.warn(f\"{pre} is not within the observation space.\")\n",
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:227: DeprecationWarning: \u001b[33mWARN: Core environment is written in old step API which returns one bool instead of two. It is recommended to rewrite the environment with new step API. \u001b[0m\n",
            "  logger.deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
            "  if not isinstance(done, (bool, np.bool8)):\n",
            "/usr/local/lib/python3.10/dist-packages/gym/core.py:49: DeprecationWarning: \u001b[33mWARN: You are calling render method, but you didn't specified the argument render_mode at environment initialization. To maintain backward compatibility, the environment will render in human mode.\n",
            "If you want to render in human mode, initialize the environment in this way: gym.make('EnvName', render_mode='human') and don't call the render method.\n",
            "See here for more information: https://www.gymlibrary.ml/content/api/\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:280: UserWarning: \u001b[33mWARN: No render modes was declared in the environment (env.metadata['render_modes'] is None or not defined), you may have trouble when calling `.render()`.\u001b[0m\n",
            "  logger.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total reward: -0.4000000000000003\n",
            "model:\n",
            "[[0.25 0.25 0.25 0.25]\n",
            " [0.25 0.25 0.25 0.25]\n",
            " [0.25 0.25 0.25 0.25]\n",
            " [0.25 0.25 0.25 0.25]\n",
            " [0.25 0.25 0.25 0.25]\n",
            " [0.25 0.25 0.25 0.25]\n",
            " [0.25 0.25 0.25 0.25]\n",
            " [0.25 0.25 0.25 0.25]\n",
            " [0.25 0.25 0.25 0.25]\n",
            " [0.25 0.25 0.25 0.25]\n",
            " [0.25 0.25 0.25 0.25]\n",
            " [0.25 0.25 0.25 0.25]\n",
            " [0.25 0.25 0.25 0.25]\n",
            " [0.25 0.25 0.25 0.25]\n",
            " [0.25 0.25 0.25 0.25]\n",
            " [0.25 0.25 0.25 0.25]\n",
            " [0.25 0.25 0.25 0.25]\n",
            " [0.25 0.25 0.25 0.25]\n",
            " [0.25 0.25 0.25 0.25]\n",
            " [0.25 0.25 0.25 0.25]\n",
            " [0.25 0.25 0.25 0.25]\n",
            " [0.25 0.25 0.25 0.25]\n",
            " [0.25 0.25 0.25 0.25]\n",
            " [0.25 0.25 0.25 0.25]\n",
            " [0.25 0.25 0.25 0.25]]\n"
          ]
        }
      ],
      "source": [
        "trajectory = get_trajectory(env, agent, max_len=100, visualize=True)\n",
        "print('total reward:', sum(trajectory['rewards']))\n",
        "print('model:')\n",
        "print(agent.model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yVsWpmk506ds"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "109adAS-IMxFzYe1yK7Y9ckN8YtxGRSnU",
      "authorship_tag": "ABX9TyNvRCAeiyHEedy3sd1sPy9L",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}